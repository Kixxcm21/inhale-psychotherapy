import streamlit as st
from huggingface_hub import InferenceClient

# è¨­å®š Hugging Face API å¯†é‘°
HF_API_TOKEN = st.secrets["HF_API_TOKEN"]  # ç¢ºä¿ä½ å·²åœ¨ Streamlit secrets ä¸­è¨­å®š
client = InferenceClient(model="HuggingFaceH4/zephyr-7b-beta", token=HF_API_TOKEN)

st.set_page_config(page_title="Inhale å¿ƒç†è«®è©¢", page_icon="ğŸ«§")
st.title("ğŸ«§ Inhale å¿ƒç†è«®è©¢")
st.caption("ç©ºæ°£ä¸­æº«ç¨”çš„è²éŸ³ã€‚ä½ å¯ä»¥èŠèŠå¿ƒäº‹ã€‚")

# ä¿ç•™å°è©±ç´€éŒ„
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åæº«æŸ”ã€æ‡‰å¿—ã€æœ‰å¿ƒç†æŒ‡å°èƒ½åŠ›çš„åŠ©ç†è«®è©¢å¸«ï¼Œæœƒä»¥æ„ŸåŒã€æ¥ç´çš„è©±èªå›æ‡‰ä½¿ç”¨è€…ï¼Œçµ¦äºˆæ‚¨æ²‰éœåŠå®‰æ…°ã€‚"}
    ]

# é¡¯ç¤ºéå»çš„è©±é¡Œ
for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

# æ¥æ”¶æ–°çš„ç”¨æˆ¶è©±é¡Œ
if user_input := st.chat_input("æˆ‘æœ‰é»æƒ³èŠèŠ..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        response = client.text_generation(
            prompt=f"<|system|>{st.session_state.messages[0]['content']}\n<|user|>{user_input}\n<|assistant|>",
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
        )
        assistant_reply = response.strip()
        st.markdown(assistant_reply)
        st.session_state.messages.append({"role": "assistant", "content": assistant_reply})


